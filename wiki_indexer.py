# -*- coding: utf-8 -*-
"""wiki-pedia

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L9K7_oBTlzM_q1Uj8LWuBU_o01enrdjE
"""



import re
import timeit
import os
import sys
import bisect 
from collections import defaultdict
from spacy.lang.en.stop_words import STOP_WORDS
from Stemmer import Stemmer
stemmer = Stemmer('porter')
import time
import nltk
from xml.sax.handler import ContentHandler
from nltk.stem.snowball import SnowballStemmer
import xml.sax
import re
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer 
from nltk.tokenize import word_tokenize

start_time = time.clock()

fpp = open(str(sys.argv[2]) + "/Title/PageMap.txt","w")
#fpp.write("abcd")
#print(fpp)

from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
def stemming(text):
  ps = PorterStemmer() 
  word_list = []
  for w in text: 
    word_list.append(ps.stem(w))
  return word_list

#def stemming(text):
    #stemmer = SnowballStemmer("english", ignore_stopwords=True)
    #return stemmer.stem(text)
#fpp = open("/content/drive/My Drive/merging_files" + "/" + "PageTitle.txt","w")
#word_count = 0
def textHandler(text):
    #print(text)
    #stop_word = {}
    #tokenizing
    text = text.encode('ascii', errors='ignore').decode()
    text = re.sub(r'[^A-Za-z0-9]+', r' ', text)
    #tokens = nltk.word_tokenize(text)#tokenizing
    #stop word removal
    #uwords = [word for word in tokens if word not in stop_word.keys()]#stop word removal
    #print('remove',uwords) 
    stop_words = set(stopwords.words('english')) 
    word_tokens = word_tokenize(text) 
    filter_sentence = [w for w in word_tokens if not w in stop_words] 
    # filter_sentence = []  
    # for w in word_tokens: 
    #     if w not in stop_words: 
    #         filter_sentence.append(w) 

    stemmer = Stemmer('porter')
    stem_text=[]
    for word in filter_sentence:
        stem_text.append(stemmer.stemWord(word))
    #print(filter_sentence)
    # print('before',len(filter_sentence))
    # print('after',len(stemming(stem_text)))
    return stem_text

def remove_puct(text):
  #print(text)
  punc = '''|!()-[]{};:'"\, <>./?@#$%^&*_~'''
  for ele in text:  
    if ele in punc:  
        text = text.replace(ele, " ")
  return text

cnt = 0
PageCount = 0
word_map = defaultdict(lambda:defaultdict(lambda:defaultdict(int)))
def create_table(text,ID,field):
  global cnt
  #print(len(word_map))
  if len(text) == 0:
    return
  #ps = PorterStemmer() 
  for word in text:
    #print(word)
    if len(word_map) == 1000:
      #print(len(word_map))
      cnt += 1
      saveToText(word_map)
      word_map.clear()
      #print("after clear",len(word_map))

    if len(word)>3:
      if word in word_map:
        if ID in word_map[word]:
          if field in word_map[word][ID]:
            word_map[word][ID][field] += 1
          else:
            word_map[word][ID][field] = 1
        else:
          word_map[word][ID] = {field:1}
      else:
        word_map[word] = dict({ID:{field:1}})
    #cnt = 0



def saveToText(word_map):
  #word_map = sorted(word_map)
  filePath = "/home/blackhat/Downloads/2019201099/Indexes/index" + str(cnt) + ".txt"
  fp = open(filePath, "w")
  toText = None
  for word in sorted(word_map):
    toText = word + ':'
    for ID in sorted(word_map[word]):
      toText += ' ' + 'd' + str(ID)
      
      if "i" in word_map[word][ID]:
        toText += 'i' + str(word_map[word][ID]["i"])
      
      if "r" in word_map[word][ID]:
        toText += 'r' + str(word_map[word][ID]["r"])
      
      if "b" in word_map[word][ID]:
        toText += 'b' + str(word_map[word][ID]["b"])
      
      if "c" in word_map[word][ID]:
        toText += 'c' + str(word_map[word][ID]["c"])
      
      if "t" in word_map[word][ID]:
        toText += 't' + str(word_map[word][ID]["t"])

      if "e" in word_map[word][ID]:
        toText += 'e' + str(word_map[word][ID]["e"])
      
    fp.write(toText + "\n")
#print(word_map)

def processdata(text,tag,id):
  #global fpp
  if tag == "title":
    #print(text)
    #fpp.write(str(id) + ":" + text)
    text = text.lower()
    text = remove_puct(text)
    #print(text)
    text = textHandler(text)
    create_table(text,id,'t')
  
  if tag == "text":
    text = text.lower()
    body = []
    infobox = re.findall(r'{{infobox(.*?)}}',text,flags=re.DOTALL)
    category = re.findall(r'\[\[category:(.*?)\]\]',text,flags=re.MULTILINE)
    reference = re.findall(r'== ?references ?==(.*?)==',text,flags=re.DOTALL)
    body = re.sub(r'\{\{.[^}}]*\}\}', r' ', text)
    external_link = re.findall(r'== external links ==(.*?)==',text,flags=re.DOTALL)
    #print(external_link)
    body = textHandler(body)
    text_category = ' '.join(category)
    text_external = ' '.join(external_link)
    text_infobox = ' '.join(infobox)
    text_reference = ' '.join(reference)
    category = textHandler(text_category)
    infobox = textHandler(text_infobox)
    reference = textHandler(text_reference)
    external_link = textHandler(text_external)
    create_table(category,id,'c')
    create_table(infobox,id,'i')
    create_table(reference,id,'r')
    create_table(body,id,'b')
    create_table(external_link,id,'e')

import os
WordLimit = 1000
def mergeTwoFiles(file_1 , file_2 , path):
    if file_1 == file_2:
        return
    temp_file = path + '/' + "temp.txt" 
    fp1 = open(file_1, 'r')
    fp2 = open(file_2, 'r')
    fp3 = open(temp_file, 'w')
    line_of_file1 = fp1.readline().strip('\n')
    line_of_file2 = fp2.readline().strip('\n')
    while (line_of_file1 and line_of_file2):
        word_of_file1 = line_of_file1.split(":")[0]
        word_of_file2 = line_of_file2.split(":")[0]
        #print("1st = ",word_of_file1)
        #print("2nd = ",word_of_file2)
        if word_of_file1 < word_of_file2:
          fp3.write(line_of_file1 + '\n')
          line_of_file1 = fp1.readline().strip('\n')

        elif word_of_file2 < word_of_file1:
          fp3.write(line_of_file2 + '\n')
          line_of_file2 = fp2.readline().strip('\n')
       
        else:
          # print("X = ",line_of_file1)
          # print(line_of_file2)
          # print(len(line_of_file2))
          list2 = line_of_file2.strip().split(':')
          list1 = line_of_file1.strip().split(":")
          if len(list1)>1 and len(list2)>1:
            fp3.write(word_of_file1 + ':' + list1[1] + list2[1] + '\n')
          elif len(list1)>1:
            fp3.write(word_of_file1 + ':' + list1[1] + '\n')
          elif len(list2)>1:
            fp3.write(word_of_file1 + ':' + list2[1] + '\n')
          line_of_file2 = fp2.readline().strip('\n')
          line_of_file1 = fp1.readline().strip('\n')

    while line_of_file1:
        fp3.write(line_of_file1 + '\n')
        line_of_file1 = fp1.readline().strip('\n')
    while line_of_file2:
        fp3.write(line_of_file2 + '\n')
        line_of_file2 = fp2.readline().strip('\n')
    os.remove(file_1)
    os.remove(file_2)
    os.rename(temp_file, file_1)

def split(file_path,path):
    global WordLimit
    idx = 0
    cnt = 0
    file1 = file_path
    secondary_index_file = path + '/' + 'secondaryIndex.txt'
    
    fp1=open(file1,'r') 
    sf =open(secondary_index_file,'w')
    
    line_of_file1 = fp1.readline().strip('\n')
    
    while( line_of_file1 ):


        if(cnt == 0):
            index_file_name = path+ '/'  + 'index_' + str(idx) +'.txt'
            wordFile1 = line_of_file1.split(":")[0]
            sf.write(wordFile1 + '\n')
            fp2 = open(index_file_name, 'w')
        
        cnt += 1
        fp2.write(line_of_file1 + '\n')
        if(WordLimit == cnt):
            idx += 1
            cnt = 0
            fp2.close()
        line_of_file1 = fp1.readline().strip('\n')
    
    fp1.close()
    fp2.close()
    sf.close()
    os.remove(file_path)

def mergeFiles(path):
    filelist = os.listdir(path)
    filelist.sort()

    while(len(filelist)>1):
        first_file = filelist[0]
        second_file = filelist[1]
        mergeTwoFiles(path + '/' + first_file , path + '/' + second_file, path)
        filelist.remove(filelist[1])
    split(path+'/'+filelist[0],path)

from xml.sax.handler import ContentHandler
import xml.sax
class XMLHelper(xml.sax.handler.ContentHandler):
  global fpp
  def init(self):
    self.CurrentData = ""
    #self.PageCount = 0
    self.text = ""
  def startElement(self, tag, attributes):
    global PageCount
    self.CurrentData = tag
    if tag == "page":
      PageCount += 1
  def endElement(self, tag):
    if self.CurrentData == "title":
      processdata(self.title,tag,PageCount)
      fpp.write(str(PageCount) + ":" + self.title.strip()+"\n")
      #print("Title : ", self.title)
      #print("TEXTOUT")
    #elif self.CurrentData == "id":
      #processdata(self.title,tag,XMLHelper.PageCount)
      #print("ID    : ", self.id)
      #print("TEXTOUT")
    elif self.CurrentData == "text":
      processdata(self.text,tag,PageCount)
      #print("Text  : ",self.text)
    self.CurrentData = ""
    self.text = ""

  def characters(self, content):
    #print(content)
    if self.CurrentData   == "title":
      self.title = content
    elif self.CurrentData == "id":
      self.id = content
    elif self.CurrentData == "text":
      self.text += content

if ( __name__ == "__main__"):
  path = sys.argv[1]
  parser = xml.sax.make_parser()
  parser.setFeature(xml.sax.handler.feature_namespaces, 0)
  handler = XMLHelper()
  parser.setContentHandler(handler)
  #parser.parse('/content/drive/My Drive/enwiki-20200801-pages-articles-multistream1.xml-p1p30303')
  parser.parse('short.xml')
  if len(word_map)>0:
    cnt += 1
    saveToText(word_map)
    word_map.clear()
  mergeFiles(path)
  file1 = open(path + "/" + "PageCount.txt", "w") 
  file1.write(str(PageCount))
  file1.close()
  fpp.close()
  print(time.clock() - start_time, "seconds")



