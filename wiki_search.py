# -*- coding: utf-8 -*-
"""search_query

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1teXwv3ayy9W7QXwPGrmHV-mST2xosuUd
"""

import re
import timeit
import os
import sys
import bisect 
import math 
import operator
from collections import defaultdict
from spacy.lang.en.stop_words import STOP_WORDS
from Stemmer import Stemmer
stemmer = Stemmer('porter')
import time
import nltk
import re
from xml.sax.handler import ContentHandler
from nltk.stem.snowball import SnowballStemmer
import xml.sax
import re
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer 
from nltk.tokenize import word_tokenize

def nonField_query(path, text,secondary_index_list):
  #print(1)
  text = text.lower()
  text = text.encode('ascii', errors='ignore').decode()
  text = re.sub(r'[^A-Za-z0-9]+', r' ', text)
  stop_words = set(stopwords.words('english'))
  word_tokens = word_tokenize(text)
  filter_sentence = [w for w in word_tokens if not w in stop_words]
  # filter_sentence = []
  # for w in word_tokens:
  #     if w not in stop_words:
  #         filter_sentence.append(w)
  stemmer = Stemmer('porter')
  stem_text=[]
  for word in filter_sentence:
      stem_text.append(stemmer.stemWord(word))
  #print(word)
  result_list = []
  #print(stem_text)
  for word in stem_text:
    result_list.append(Posting(secondary_index_list,word,path))
  return result_list

def Posting(secondary_index_list,Word,path):
  file_Id = bisect.bisect(secondary_index_list, Word)
  posting_list = ""
  if file_Id > 0:
    file_Id = file_Id-1 
  #print("file no = ",file_Id)
  fp = open(path + "/Indexes/index_" +str(file_Id)+".txt", "r")
  for data in fp:
    text = data.split(":")
    #print("text1 = ",text)
    if text[0] == Word:
      for i in range(0,len(text)):
        if(i == 0):
          posting_list += text[i]+':'
        else:  
          posting_list+=text[i]
        
  fp.close()
  posting_list = posting_list[0:len(posting_list)-1]
  return posting_list

def getquery(line):
  line= line.split(',')
  k = line[0].strip()
  query = line[1].strip()
  return k,query

def check_query(text):
  if ":" in text:
    k,query = getquery(text)
    return k,query,True
  else:
    #non-field
    #print("enter")
    k,query = getquery(text)
    #list1 = nonField_query(path,text,secondary_index_list)
    return k,query,False

def regex_sep_page(word):
  word_list = word.split()
  page_list = []
  for i in range(0,len(word_list)):
    docFreq = re.findall(r'd[0-9]+',word_list[i])
    docFreq = ''.join([str(elem) for elem in docFreq]) 
    page_list.append(docFreq[1:])
  #print(page_list)
  return page_list

word_map_page = defaultdict(list)
def create_map_for_page(word_list):
  word_map_page[word_list[0]] = regex_sep_page(word_list[1])

word_map_freq = defaultdict(lambda:defaultdict(int))
def create_map_for_freq(word_list,flag,types):
  if flag == True:
    word = word_list[0]
    token_each = word_list[1].split()
    for i in token_each:
      docFreq = re.findall(r'd[0-9]+',i)
      docFreq = ''.join([str(elem) for elem in docFreq])
      page_no = docFreq[1:]
      r_list = re.findall(r'r[0-9]+',i)
      e_list = re.findall(r'e[0-9]+',i)
      b_list = re.findall(r'b[0-9]+',i)
      t_list = re.findall(r't[0-9]+',i)
      i_list = re.findall(r'i[0-9]+',i)
      c_list = re.findall(r'c[0-9]+',i)

      if r_list:
        x = ''.join([str(elem) for elem in r_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt
      
      if e_list:
        x = ''.join([str(elem) for elem in e_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      if b_list:
        x = ''.join([str(elem) for elem in b_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      if t_list:
        x = ''.join([str(elem) for elem in t_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      if i_list:
        x = ''.join([str(elem) for elem in i_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      if c_list:
        x = ''.join([str(elem) for elem in c_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

  else:
    word = word_list[0]
    token_each = word_list[1].split()
    for i in token_each:
      docFreq = re.findall(r'd[0-9]+',i)
      docFreq = ''.join([str(elem) for elem in docFreq])
      page_no = docFreq[1:]
      r_list = re.findall(r'r[0-9]+',i)
      e_list = re.findall(r'e[0-9]+',i)
      b_list = re.findall(r'b[0-9]+',i)
      t_list = re.findall(r't[0-9]+',i)
      i_list = re.findall(r'i[0-9]+',i)
      c_list = re.findall(r'c[0-9]+',i)

      if r_list and types == "r":
        x = ''.join([str(elem) for elem in r_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt
      
      elif e_list and types == "e":
        x = ''.join([str(elem) for elem in e_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      elif b_list and types == "b":
        x = ''.join([str(elem) for elem in b_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      elif t_list and types == "t":
        x = ''.join([str(elem) for elem in t_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      elif i_list and types == "i":
        x = ''.join([str(elem) for elem in i_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

      elif c_list and types == "c":
        x = ''.join([str(elem) for elem in c_list])
        cnt = int(x[1:])
        word_map_freq[word][page_no] += cnt

def remove_posting(posting_list,flag,type):
  for i in range(0,len(posting_list)):
    if len(posting_list[i])>0:
      x = posting_list[i].split(":")
      create_map_for_page(x)
      create_map_for_freq(x,flag,types)

word_map_idf = defaultdict(float)
def calculate_idf(path): 
  file = open(path + "/Indexes/PageCount.txt",'r') 
  len_x = int(file.readline().strip('\n'))
  for i in word_map_page:
    x = len(word_map_page[i])
    y = math.log(float(len_x/x))
    #print(y)
    word_map_idf[i] = y
    #print("idf = ",word_map_idf[i])

def intersection_unoin(k):
  d = []
  for i in word_map_page:
    d.append(word_map_page[i])
  page_list = set.intersection(*[set(x) for x in d])
  page_list = list(page_list)
  list_union = set().union(*d)
  list_union = list(list_union)
  if len(page_list) >= int(k):
    return page_list
  else:
    return list_union

map_tfidf = defaultdict(float)
def tfidf(idf_list):
  for x in idf_list:
    ans = 0
    for i in word_map_freq:
      if x in word_map_freq[i]:
        y = 1+word_map_freq[i][x]
        z = word_map_idf[i]
        #print("value of z = ",z)
        ans += float(float(z*(math.log(y))))
    map_tfidf[x] = ans

def sortByScores(docScoreDict):
  temp = dict(sorted(docScoreDict.items(), key=operator.itemgetter(1),reverse=True))
  return list(temp.keys())

def split_field_query(query):
  query_tokens = query.split(":")
  query_types = []
  query_values = []
  i=0
  for token in query_tokens:
      if i == 0:
          query_types.append(token[0:1])
          i += 1
          continue
      if i == len(query_tokens)-1:
          query_values.append(token)
          i += 1
          continue
      query_types.append(token[token.rfind(" ")+1:len(token)][0:1])
      query_values.append(token[0:token.rfind(" ")])
      i+=1
  return query_types,query_values

def intersection_unoin_field(d,k):
  for i in word_map_page:
    d.append(word_map_page[i])
  page_list = set.intersection(*[set(x) for x in d])
  page_list = list(page_list)
  list_union = set().union(*d)
  list_union = list(list_union)
  if len(page_list) >= int(k):
    return page_list
  else:
    return list_union

if (__name__ == "__main__"):
    path = sys.argv[1]
    with open(path + "/Indexes/secondaryIndex.txt") as f:
      secondary_index_list = f.read().splitlines()
    fpp = open(path + "/Title/query_op.txt","w")
    fpx = open(path + "/query.txt","r")
    for l in fpx:
      query = l.strip()
      #query = "3,modern monarchi"
      k,query,flag = check_query(query)
      types = []
      query_value = []
      posting_list = []
      all_list_field = []
      top_k = []
      if flag == True:
        types,query_value = split_field_query(query)
        for i in range(0,len(types)):
          posting_list = nonField_query(path,query_value[i],secondary_index_list)
          remove_posting(posting_list,True,types[i])
          #print(posting_list)
          calculate_idf(path)
          #print("len = ",len(word_map_freq))
          idf_list = intersection_unoin(k)
          tfidf(idf_list)
          # print(idf_list)
          #print(len(map_tfidf))
          top_k = sortByScores(map_tfidf)
          #print(top_k)
          all_list_field.append(top_k)
          word_map_freq.clear()
          word_map_idf.clear()
          word_map_page.clear()
          map_tfidf.clear()
        top_k = intersection_unoin_field(all_list_field,k)
        #print(all_list_field)
      else:
        posting_list = nonField_query(path,query,secondary_index_list)
        remove_posting(posting_list,False,"n")
        calculate_idf(path)
      #K = 2 #as a argument
        idf_list = intersection_unoin(k)
        #print(idf_list)
        tfidf(idf_list)
        top_k = sortByScores(map_tfidf)
        word_map_freq.clear()
        word_map_idf.clear()
        word_map_page.clear()
        map_tfidf.clear()
      #print(top_k)
      for i in range(0,len(top_k)):
        if i == int(k):
          break
        with open(path + "/Title/PageMap.txt", "r") as a_file:
          for line in a_file:
            stripped_line = line.strip()
            p = stripped_line.split(":")
            if p[0] == top_k[i]:
              #print(top_k[i])
              fpp.write(stripped_line+"\n")
              break

      fpp.close()


